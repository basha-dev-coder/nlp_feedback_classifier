{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['discourse_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "tokenizer = Tokenizer()\n",
    "texts = [x.lower() for x in train_df['discourse_text'].values]\n",
    "tokenizer.fit_on_texts(texts)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "# print(tokenizer.word_index)\n",
    "# print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36765/36765 [01:55<00:00, 319.38it/s]\n"
     ]
    }
   ],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for text in tqdm(texts):\n",
    "    # print(text)\n",
    "    token_list = tokenizer.texts_to_sequences(text)\n",
    "    # print(token_list)\n",
    "    for i in range(1,len(token_list)):\n",
    "        ngram_sequences = token_list[:i+1]\n",
    "        input_sequences.append(ngram_sequences)\n",
    "\n",
    "max_sequences_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36765/36765 [06:07<00:00, 99.97it/s] \n"
     ]
    }
   ],
   "source": [
    "texts_tensor = []\n",
    "for x in tqdm(train_df['discourse_text']):\n",
    "    texts_tensor.append(nlp(x).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tensor = tf.reshape(texts_tensor,[36765,1,300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 300), dtype=float32, numpy=\n",
       "array([[[-1.7310259 ,  1.2019125 , -3.6277354 , ...,  0.0363916 ,\n",
       "         -2.994872  ,  1.4502984 ],\n",
       "        [-0.8571301 ,  1.7046484 , -3.1711113 , ..., -0.6690958 ,\n",
       "         -4.5095615 ,  1.7122338 ],\n",
       "        [-0.7345443 ,  3.1622272 , -2.7153568 , ...,  0.02443947,\n",
       "         -3.869576  ,  2.00546   ],\n",
       "        ...,\n",
       "        [-3.285236  ,  0.88648003, -2.619274  , ..., -0.62785   ,\n",
       "         -2.2943168 ,  4.129184  ],\n",
       "        [-1.8806963 ,  2.1315725 , -3.1053443 , ..., -0.08605026,\n",
       "         -2.4896052 ,  2.1499102 ],\n",
       "        [-2.3043728 ,  0.34056365, -2.421952  , ..., -1.7717329 ,\n",
       "         -2.6810198 ,  0.9267128 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in texts_tensor:\n",
    "    print(x.shape)\n",
    "    for y in x:\n",
    "        print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# model.add(keras.layers.Embedding(total_words, 100, input_length=max_sequences_len-1))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300,return_sequences=True)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "adam = tf.keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = keras.layers.Bidirectional(keras.layers.LSTM(150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df['discourse_effectiveness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(Keys,values,features):\n",
    "    init = tf.lookup.KeyValueTensorInitializer(Keys,values)\n",
    "    table = tf.lookup.StaticHashTable(init,default_value=-1)\n",
    "    return table.lookup(tf.constant(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = tf.constant(['Adequate', 'Effective','Ineffective'])\n",
    "\n",
    "TARGET_LABELS = tf.constant([0,1,2])\n",
    "y_train = tf.one_hot(get_label( TARGET, TARGET_LABELS, labels[:200]),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_batch = tf.one_hot(get_label( TARGET, TARGET_LABELS, labels),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([36765, 3])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.reshape(y_train,[10,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ys = tf.keras.utils.to_categorical(labels[:10], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([36765, 1, 300])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29412.0"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36765 - 36765 * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "920/920 [==============================] - 14s 15ms/step - loss: 0.8244 - accuracy: 0.6297 - val_loss: 0.8369 - val_accuracy: 0.6154\n",
      "Epoch 2/100\n",
      "920/920 [==============================] - 14s 15ms/step - loss: 0.8110 - accuracy: 0.6355 - val_loss: 0.8736 - val_accuracy: 0.6119\n",
      "Epoch 3/100\n",
      "920/920 [==============================] - 14s 15ms/step - loss: 0.8007 - accuracy: 0.6389 - val_loss: 0.8775 - val_accuracy: 0.6047\n",
      "Epoch 4/100\n",
      "920/920 [==============================] - 15s 16ms/step - loss: 0.7960 - accuracy: 0.6431 - val_loss: 0.8793 - val_accuracy: 0.6087\n",
      "Epoch 5/100\n",
      "920/920 [==============================] - 14s 15ms/step - loss: 0.7884 - accuracy: 0.6455 - val_loss: 0.8539 - val_accuracy: 0.6108\n",
      "Epoch 6/100\n",
      "920/920 [==============================] - 14s 16ms/step - loss: 0.7791 - accuracy: 0.6515 - val_loss: 0.8550 - val_accuracy: 0.6219\n",
      "Epoch 7/100\n",
      "300/920 [========>.....................] - ETA: 9s - loss: 0.7851 - accuracy: 0.6456"
     ]
    }
   ],
   "source": [
    "# layers(tf.reshape(text_1,[1,1,300]))\n",
    "model.fit(texts_tensor[:29412],y_train_batch[:29412],epochs=100,verbose=1,validation_data=(texts_tensor[29412:],y_train_batch[29412:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([200, 3])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batches = tf.data.Dataset.from_tensor_slices(texts_tensor).batch(32)\n",
    "y_batches = tf.data.Dataset.from_tensor_slices(y_train).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(X_b):\n",
    "    # print((X_b)))\n",
    "    print(X_b.apply(lambda x : x))\n",
    "    # X_b.map(lambda x : nlp(str(x).vector))\n",
    "    # return nlp.(X_b).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = tf.data.Dataset.from_tensor_slices(train_df['discourse_text'].values).batch(32)\n",
    "train_y_batches = tf.data.Dataset.from_tensor_slices(y_train_batch).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_batch = tf.data.Dataset.zip((train_batches,train_y_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=TensorSpec(shape=(None, None), dtype=tf.string, name=None)>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`transformation_func` must return a `tf.data.Dataset` object. Got <class 'NoneType'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Automation\\python\\nlp_feedback_predict\\NLP_Classifier.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Automation/python/nlp_feedback_predict/NLP_Classifier.ipynb#ch0000040?line=0'>1</a>\u001b[0m train_batches\u001b[39m.\u001b[39;49mbatch(\u001b[39m32\u001b[39;49m)\u001b[39m.\u001b[39;49mapply(get_vector)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2272\u001b[0m, in \u001b[0;36mDatasetV2.apply\u001b[1;34m(self, transformation_func)\u001b[0m\n\u001b[0;32m   2270\u001b[0m dataset \u001b[39m=\u001b[39m transformation_func(\u001b[39mself\u001b[39m)\n\u001b[0;32m   2271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset, DatasetV2):\n\u001b[1;32m-> 2272\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2273\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`transformation_func` must return a `tf.data.Dataset` object. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2274\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(dataset)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2275\u001b[0m dataset\u001b[39m.\u001b[39m_input_datasets \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m]  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "\u001b[1;31mTypeError\u001b[0m: `transformation_func` must return a `tf.data.Dataset` object. Got <class 'NoneType'>."
     ]
    }
   ],
   "source": [
    "train_batches.batch(32).apply(get_vector)\n",
    "# list(dataset_batch.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = tf.data.Dataset.zip( (text_batches , y_batches) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_batch, y_batch in data_set.batch(10):\n",
    "    for x in x_batch:\n",
    "        for y in x:\n",
    "            print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.random.normal([32, 10, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset element_spec=(TensorSpec(shape=(None, 2, 300), dtype=tf.float32, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 8), dtype=float32, numpy=\n",
       "array([[-1.8753177 ,  0.39924157, -1.3061185 , -0.09128398, -1.8712283 ,\n",
       "        -0.34483775, -0.57712895,  1.4799764 ],\n",
       "       [ 0.11513742, -1.3900502 ,  0.20380816,  0.6482683 , -1.6172249 ,\n",
       "         0.3066741 ,  0.07384012, -0.9795225 ],\n",
       "       [ 0.87422425,  0.78189695,  0.01507624,  0.17949   , -1.6737931 ,\n",
       "        -0.45730028, -0.81652325,  0.3563014 ],\n",
       "       [-0.19982137,  0.84976166, -0.59731007, -0.96287096, -0.6522744 ,\n",
       "        -0.16280177, -1.1096549 ,  1.2835126 ],\n",
       "       [ 1.3445842 , -0.97756916, -0.31488225,  0.58931786, -0.13619326,\n",
       "         1.5717092 ,  0.32090935,  0.80906075],\n",
       "       [ 1.4670334 , -0.119016  , -0.06259945,  1.2847294 , -1.0686811 ,\n",
       "        -0.51476103, -0.40006658,  0.66146886],\n",
       "       [ 1.060258  ,  0.70658004,  0.45708534, -1.5078332 , -1.2828417 ,\n",
       "        -0.70203847,  0.09950235, -1.3442135 ],\n",
       "       [ 1.0623285 ,  0.75111175, -1.0949779 ,  0.32393587, -1.5980583 ,\n",
       "        -1.4131682 ,  0.32177702,  0.06165824],\n",
       "       [-1.1097913 , -0.5550918 , -0.19037557, -1.2704947 , -1.957086  ,\n",
       "        -1.2541072 ,  1.2266876 , -0.60341763],\n",
       "       [-0.29385883, -0.4129542 , -0.4209914 , -0.09793455,  1.1722451 ,\n",
       "         0.15445971, -1.058159  ,  1.9375452 ]], dtype=float32)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccc9907020a360c0774b7240d7189d7caac06a5804f44f1a6c385a36278d3a41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
